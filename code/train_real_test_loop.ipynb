{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imrotate\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import preprocess_input, InceptionV3\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from time import gmtime, strftime\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "root_dir = os.path.abspath('..')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fetch_real_data(base_dir,num_subjects):\n",
    "    '''Load in simulated data and motion files.'''\n",
    "    subject_list = os.listdir((os.path.join(base_dir)))\n",
    "    subject_list = [item for item in subject_list if item.startswith('sub') == True] #Filter everything but subjects\n",
    "    subject_list = sorted(subject_list) #sort in numerical order to make OS independent\n",
    "    counter = 0\n",
    "    num_vols=172\n",
    "    X = np.zeros((num_vols*num_subjects,128,128,64))\n",
    "    y = np.zeros(num_vols*num_subjects)\n",
    "    X_subject = np.zeros((128,128,64,num_vols))\n",
    "    y_subject = np.zeros(num_vols)\n",
    "    for subject_index, subject_number in enumerate(subject_list):\n",
    "        if subject_index < num_subjects:\n",
    "            data_path = os.path.join(base_dir,subject_number,'dwi.nii.gz')\n",
    "            if os.path.isfile(data_path):\n",
    "                data_header = nib.load(data_path)\n",
    "                X_subject = data_header.get_data()\n",
    "                y_subject = np.load(os.path.join(base_dir,subject_number,'y_manual.npy'))\n",
    "                start_index = counter*num_vols\n",
    "                end_index = (counter+1)*num_vols\n",
    "                X[start_index:end_index,:] = np.moveaxis(X_subject,3,0)\n",
    "                y[start_index:end_index] = y_subject\n",
    "                counter += 1\n",
    "    return X,y\n",
    "\n",
    "def preprocess_data_coronal(X,target_height=299,target_width=299, base_slice=64,rescale=False,):\n",
    "    '''Convert each MR volume to three slices through a single plane, scales the data and resamples\n",
    "    to 299 by 299 pixels. Optionally performs augmentation.'''   \n",
    "    #slices = [22,36,50] #Planes to slice\n",
    "    slices = np.array([base_slice,base_slice,base_slice]) #Planes to slice\n",
    "    pad_max = np.max([X.shape[1],X.shape[3]]) #Width to pad images to\n",
    "    \n",
    "    num_volumes = X.shape[0]\n",
    "    height = X.shape[1]\n",
    "    width = X.shape[3]\n",
    "    num_slices = X.shape[2]\n",
    "    X_preprocessed = np.zeros((num_volumes,target_height,target_width,3))\n",
    "\n",
    "    for i in range(num_volumes):\n",
    "        for j in range(3):\n",
    "            if (j == 0):\n",
    "                X_preprocessed[i,:,:,j] = imresize(pad_image(np.squeeze(X[i,:,slices[j],:]),pad_max),(target_width,target_height))\n",
    "            if (j == 1):\n",
    "                X_preprocessed[i,:,:,j] = imresize(pad_image(np.squeeze(X[i,:,slices[j],:]),pad_max),(target_width,target_height))\n",
    "            if (j == 2):\n",
    "                X_preprocessed[i,:,:,j] = imresize(pad_image(np.squeeze(X[i,:,slices[j],:]),pad_max),(target_width,target_height))     \n",
    "   \n",
    "    if rescale == True:\n",
    "        X_preprocessed = X_preprocessed.astype(np.float32)\n",
    "        X_preprocessed/= 255\n",
    "        percentile =np.percentile(X_preprocessed,99.9)\n",
    "        X_preprocessed[X_preprocessed>percentile] = percentile\n",
    "        X_preprocessed/=percentile\n",
    "        X_preprocessed -= 0.5\n",
    "        X_preprocessed *= 2.\n",
    "    return X_preprocessed\n",
    "\n",
    "def preprocess_data_saggital(X,target_height=299,target_width=299, base_slice=64,rescale=False,):\n",
    "    '''Convert each MR volume to three slices through a single plane, scales the data and resamples\n",
    "    to 299 by 299 pixels. Optionally performs augmentation.'''   \n",
    "    #slices = [22,36,50] #Planes to slice\n",
    "    slices = np.array([base_slice,base_slice,base_slice]) #Planes to slice\n",
    "    pad_max = np.max([X.shape[1],X.shape[3]]) #Width to pad images to\n",
    "    \n",
    "    num_volumes = X.shape[0]\n",
    "    height = X.shape[1]\n",
    "    width = X.shape[2]\n",
    "    num_slices = X.shape[3]\n",
    "    X_preprocessed = np.zeros((num_volumes,target_height,target_width,3))\n",
    "\n",
    "    for i in range(num_volumes):\n",
    "        for j in range(3):\n",
    "            if (j == 0):\n",
    "                X_preprocessed[i,:,:,j] = imresize(pad_image(np.squeeze(X[i,slices[j],:,:]),pad_max),(target_width,target_height))\n",
    "            if (j == 1):\n",
    "                X_preprocessed[i,:,:,j] = imresize(pad_image(np.squeeze(X[i,slices[j],:,:]),pad_max),(target_width,target_height))\n",
    "            if (j == 2):\n",
    "                X_preprocessed[i,:,:,j] = imresize(pad_image(np.squeeze(X[i,slices[j],:,:]),pad_max),(target_width,target_height))     \n",
    "   \n",
    "    if rescale == True:\n",
    "        X_preprocessed = X_preprocessed.astype(np.float32)\n",
    "        X_preprocessed/= 255\n",
    "        percentile =np.percentile(X_preprocessed,99.9)\n",
    "        X_preprocessed[X_preprocessed>percentile] = percentile\n",
    "        X_preprocessed/=percentile\n",
    "        X_preprocessed -= 0.5\n",
    "        X_preprocessed *= 2.\n",
    "    return X_preprocessed\n",
    "\n",
    "def pad_image(image,pad_max):\n",
    "    if pad_max == 0:\n",
    "        return image\n",
    "    else:\n",
    "        pad_width = np.array([[pad_max,pad_max],[pad_max,pad_max]])-[image.shape,image.shape]\n",
    "        pad_width=np.transpose(pad_width)\n",
    "        pad_width[:,0] = np.floor(pad_width[:,0]/2)\n",
    "        pad_width[:,1] = np.ceil(pad_width[:,1]/2)\n",
    "        return np.lib.pad(image,pad_width,'constant',constant_values=(0))\n",
    "    \n",
    "def preprocess_input_scaling(x):\n",
    "        x=x.astype(np.float32)\n",
    "        x /= 255.\n",
    "        percentile =np.percentile(x,99.9)\n",
    "        x[x>percentile] = percentile\n",
    "        x/=percentile\n",
    "        x -= 0.5\n",
    "        x *= 2.\n",
    "        return x\n",
    "    \n",
    "def save_model(model,filepath):\n",
    "    if not os.path.isfile(filepath):\n",
    "        model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data_for_training(view,num_train=5,num_test=2):\n",
    "    #view = 'coronal'\n",
    "    #view = 'saggital'\n",
    "\n",
    "    num_to_fetch = num_train + num_test\n",
    "    X,y = fetch_real_data('../data/sourcedata/',num_to_fetch)\n",
    "    print(X.shape,y.shape)\n",
    "\n",
    "    num_vols = 172\n",
    "\n",
    "    slices_to_extract = [50,64,80]\n",
    "\n",
    "    X_train = np.zeros((num_vols*num_train*len(slices_to_extract),299,299,3))\n",
    "    X_test = np.zeros((num_vols*num_test*len(slices_to_extract),299,299,3))\n",
    "    y_train = np.zeros((num_vols*num_train*len(slices_to_extract)))\n",
    "    y_test = np.zeros((num_vols*num_test*len(slices_to_extract)))\n",
    "\n",
    "    for slice_num, slice_indx in enumerate(slices_to_extract):\n",
    "        start_indx_train = slice_num * num_vols*num_train\n",
    "        end_indx_train = (slice_num+1) * num_vols*num_train\n",
    "        start_indx_test = slice_num * num_vols*num_test\n",
    "        end_indx_test = (slice_num+1) * num_vols*num_test\n",
    "        if view == 'saggital':\n",
    "            X_preprocessed = preprocess_data_saggital(X,base_slice=slice_indx)\n",
    "        elif view == 'coronal':\n",
    "            X_preprocessed = preprocess_data_coronal(X,base_slice=slice_indx)\n",
    "        else:\n",
    "            print('View not recognised')    \n",
    "        X_train[start_indx_train:end_indx_train,:] = X_preprocessed[:num_vols*num_train]\n",
    "        X_test[start_indx_test:end_indx_test,:] = X_preprocessed[num_vols*num_train:]\n",
    "        y_train[start_indx_train:end_indx_train]= y[:num_vols*num_train]\n",
    "        y_test[start_indx_test:end_indx_test]=y[num_vols*num_train:]\n",
    "\n",
    "    y_train = (y_train!=0)\n",
    "    y_test = (y_test!=0)\n",
    "\n",
    "    print('Train shape X',X_train.shape)\n",
    "    print('Train shape y',X_train.shape)\n",
    "\n",
    "    print('Test shape X',X_test.shape)\n",
    "    print('Test shape y',y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    #Set up inception v3 for transfer learning\n",
    "    base_model = InceptionV3(weights='imagenet',include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(16,activation='relu')(x)\n",
    "    predictions = Dense(2,activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input,outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test,model,num_epochs=30):\n",
    "    train_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_scaling,\n",
    "    data_format='channels_last')\n",
    "\n",
    "    validation_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_scaling,\n",
    "    data_format='channels_last',)\n",
    "\n",
    "    train_batch_size = 43\n",
    "    validation_batch_size = 43\n",
    "    train_examples = X_train.shape[0]\n",
    "    validation_examples = X_test.shape[0]\n",
    "    train_data = train_generator.flow(X_train,to_categorical(y_train,2),batch_size=train_batch_size,shuffle=True)\n",
    "    validation_data = validation_generator.flow(X_test,to_categorical(y_test,2),batch_size=validation_batch_size,shuffle=False)\n",
    "    now = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    filename = 'keras_logs/real_'+now+'.epoch{epoch:02d}-lossval{val_loss:.2f}.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath=filename,\n",
    "                            period=10)\n",
    "\n",
    "    train_steps_per_epoch = train_examples/train_batch_size\n",
    "    validation_steps_per_epoch = validation_examples/validation_batch_size\n",
    "    print(train_steps_per_epoch)\n",
    "    print(validation_steps_per_epoch)\n",
    "    print('Model name:','real_keras_logs/'+now)\n",
    "    \n",
    "    history = model.fit_generator(generator = train_data,\n",
    "                           steps_per_epoch=train_steps_per_epoch,\n",
    "                           epochs = num_epochs,\n",
    "                           validation_data = validation_data,\n",
    "                           validation_steps = validation_steps_per_epoch,\n",
    "                           class_weight=[1,5],\n",
    "                            callbacks=[checkpoint])\n",
    "    \n",
    "    validation_data = validation_generator.flow(X_test,to_categorical(y_test,2),batch_size=validation_batch_size,shuffle=False)\n",
    "    y_pred = model.predict_generator(validation_data,validation_steps_per_epoch)[:,1] > 0.5\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_model(X_test,y_test,model_coronal,model_saggital,num_slices):\n",
    "    validation_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_scaling,\n",
    "    data_format='channels_last',)\n",
    "\n",
    "    validation_batch_size = 43\n",
    "    num_validation_steps = X_test.shape[0]/validation_batch_size\n",
    "    model_predictions_coronal = np.zeros((X_test.shape[0],num_slices))\n",
    "    model_predictions_saggital = np.zeros((X_test.shape[0],num_slices))\n",
    "    y_test_fake = np.ones((X_test.shape[0]))\n",
    "\n",
    "    for i in range(num_slices):\n",
    "#         #Coronal\n",
    "#         X_test_slice = preprocess_data_coronal(X_test[:,10:110,:,:],base_slice = 20+2*i)  \n",
    "#         #Redefine validation generator to reset\n",
    "#         validation_data_for_testing = validation_generator.flow(X_test_slice,y_test_fake,batch_size=validation_batch_size,shuffle=False)\n",
    "#         model_predictions_coronal[:,i] = model_coronal.predict_generator(validation_data_for_testing,num_validation_steps)[:,1]\n",
    "\n",
    "        #saggital\n",
    "        X_test_slice = preprocess_data_saggital(X_test,base_slice = 35+2*i)  \n",
    "        #Redefine validation generator to reset\n",
    "        validation_data_for_testing = validation_generator.flow(X_test_slice,y_test_fake,batch_size=validation_batch_size,shuffle=False)\n",
    "        model_predictions_saggital[:,i] = model_saggital.predict_generator(validation_data_for_testing,num_validation_steps)[:,1]\n",
    "        if i % 5 ==0:\n",
    "            print('Slices complete:',i)\n",
    "    #predictions_combined = np.concatenate((model_predictions_coronal,model_predictions_saggital),axis=1)\n",
    "    predictions_combined=model_predictions_saggital\n",
    "    return(predictions_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-05466ecf95eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Fetch training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_saggital\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_saggital\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_saggital\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_saggital\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saggital'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_coronal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_coronal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_coronal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_coronal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'coronal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Fetch test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#X_test,y_test = fetch_real_data('../data/sourcedata/test/',3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b56edd46d646>\u001b[0m in \u001b[0;36mget_data_for_training\u001b[0;34m(view, num_train, num_test)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_to_fetch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_real_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/sourcedata/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_to_fetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-87c0668d6a46>\u001b[0m in \u001b[0;36mfetch_real_data\u001b[0;34m(base_dir, num_subjects)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mdata_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mX_subject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0my_subject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubject_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y_manual.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_vols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/site-packages/nibabel/dataobj_images.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, caching)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fill'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# Read array and scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unscaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_read_scaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36mget_unscaled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                        \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                        mmap=self._mmap)\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/site-packages/nibabel/volumeutils.py\u001b[0m in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'readinto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mdata_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mn_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mneeds_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    481\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muncompress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncompress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muncompress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/src/anaconda3/lib/python3.5/gzip.py\u001b[0m in \u001b[0;36m_add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fetch training data\n",
    "X_train_saggital, X_val_saggital, y_train_saggital, y_val_saggital = get_data_for_training('saggital')\n",
    "X_train_coronal, X_val_coronal, y_train_coronal, y_val_coronal = get_data_for_training('coronal')\n",
    "#Fetch test data\n",
    "#X_test,y_test = fetch_real_data('../data/sourcedata/test/',3)\n",
    "\n",
    "print('Baseline performance for val data:',1-np.sum(y_val_coronal)/np.shape(y_val_coronal))\n",
    "#print('Baseline performance for test data:',1-np.sum(y_test)/np.shape(y_test))\n",
    "\n",
    "num_steps=5\n",
    "num_slices = 30\n",
    "\n",
    "#predictions_combined_all=np.zeros((num_steps,X_test.shape[0],num_slices*2))\n",
    "\n",
    "for i in range(1,num_steps+1):\n",
    "    model = setup_model()\n",
    "    model_trained_saggital = train_model(X_train_saggital[:172*3*i],X_val_saggital,y_train_saggital[:172*3*i],y_val_saggital,model,20)\n",
    "    save_model(model_trained_saggital,('keras_logs/saggital_'+str(i)+'.h5'))\n",
    "    model = setup_model()\n",
    "    model_trained_coronal = train_model(X_train_coronal[:172*3*i],X_val_coronal,y_train_coronal[:172*3*i],y_val_coronal,model,20)\n",
    "    save_model(model_trained_coronal,('keras_logs/coronal_'+str(i)+'.h5'))\n",
    "    \n",
    "    #predictions_combined_all[i-1,:,:] = test_model(X_test,y_test,model_trained_coronal,model_trained_saggital,num_slices)\n",
    "    #print(i)\n",
    "    #y_pred = np.mean(predictions_combined_all[i-1,:])>0.5\n",
    "    #print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.99      0.99       455\n",
      "       True       0.94      0.98      0.96        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99       516\n",
      "\n",
      "[[451   4]\n",
      " [  1  60]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.mean(predictions_combined_all[1,:,:],axis=1)>0.1\n",
    "print(classification_report(y_test!=0,y_pred))\n",
    "print(confusion_matrix(y_test!=0,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slices complete: 0\n",
      "Slices complete: 5\n",
      "Slices complete: 10\n",
      "Slices complete: 15\n",
      "Slices complete: 20\n",
      "Slices complete: 25\n",
      "1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.94      1.00      0.97       455\n",
      "       True       1.00      0.56      0.72        61\n",
      "\n",
      "avg / total       0.95      0.95      0.94       516\n",
      "\n",
      "[[455   0]\n",
      " [ 27  34]]\n",
      "Slices complete: 0\n",
      "Slices complete: 5\n",
      "Slices complete: 10\n",
      "Slices complete: 15\n",
      "Slices complete: 20\n",
      "Slices complete: 25\n",
      "2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.94      1.00      0.97       455\n",
      "       True       1.00      0.54      0.70        61\n",
      "\n",
      "avg / total       0.95      0.95      0.94       516\n",
      "\n",
      "[[455   0]\n",
      " [ 28  33]]\n",
      "Slices complete: 0\n",
      "Slices complete: 5\n",
      "Slices complete: 10\n",
      "Slices complete: 15\n",
      "Slices complete: 20\n",
      "Slices complete: 25\n",
      "3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      1.00      1.00       455\n",
      "       True       1.00      0.93      0.97        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99       516\n",
      "\n",
      "[[455   0]\n",
      " [  4  57]]\n",
      "Slices complete: 0\n",
      "Slices complete: 5\n",
      "Slices complete: 10\n",
      "Slices complete: 15\n",
      "Slices complete: 20\n",
      "Slices complete: 25\n",
      "4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      1.00      1.00       455\n",
      "       True       0.98      0.95      0.97        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99       516\n",
      "\n",
      "[[454   1]\n",
      " [  3  58]]\n",
      "Slices complete: 0\n",
      "Slices complete: 5\n",
      "Slices complete: 10\n",
      "Slices complete: 15\n",
      "Slices complete: 20\n",
      "Slices complete: 25\n",
      "5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.99      1.00      1.00       455\n",
      "       True       1.00      0.95      0.97        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99       516\n",
      "\n",
      "[[455   0]\n",
      " [  3  58]]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "#Fetch test data\n",
    "X_test,y_test = fetch_real_data('../data/sourcedata/test/',3)\n",
    "#X_test = X[8*172:,:,:,:]\n",
    "#y_test = y[8*172:]\n",
    "#print(X.shape,X_test.shape)\n",
    "#print(y.shape,y_test.shape)\n",
    "#del X,y\n",
    "\n",
    "num_steps=5\n",
    "num_slices = 30\n",
    "\n",
    "predictions_combined_all=np.zeros((num_steps,X_test.shape[0],num_slices))\n",
    "\n",
    "for i in range(1,num_steps+1):\n",
    "    model_trained_saggital = models.load_model('keras_logs/saggital_'+str(i)+'.h5')\n",
    "    model_trained_coronal = models.load_model('keras_logs/coronal_'+str(i)+'.h5')\n",
    "    \n",
    "    predictions_combined_all[i-1,:,:] = test_model(X_test,y_test,model_trained_coronal,model_trained_saggital,num_slices)\n",
    "    predictions_combined = np.squeeze(predictions_combined_all[i-1,:,:])\n",
    "    y_pred = np.mean(predictions_combined,axis=1)>0.5\n",
    "    print(i)\n",
    "    print(classification_report(y_test!=0,y_pred))\n",
    "    print(confusion_matrix(y_test!=0,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('misc_files/predictions_all_real_steps.npy',predictions_combined_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predictions_combined = np.squeeze(predictions_combined_all[i-1,:,:])\n",
    "y_pred = np.mean(predictions_combined,axis=1)>0.5\n",
    "print(i)\n",
    "print(classification_report(y_test!=0,y_pred))\n",
    "print(confusion_matrix(y_test!=0,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Visualise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load back in results\n",
    "y_predictions_combined = np.load('misc_files/predictions_combined_val.npy')\n",
    "y_val = np.load('misc_files/y_val.npy')\n",
    "svm_classifier = joblib.load('misc_files/svm_classifier.pkl') \n",
    "#y_pred_val = svm_classifier.predict(y_predictions_combined)\n",
    "y_pred_val = np.mean(y_predictions_combined,axis=1) > 0.5 \n",
    "print(confusion_matrix(y_val,y_pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from matplotlib import animation\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "ind_true_pos = [indx for indx,y in enumerate(y_val) if (y_val[indx] == 1) and (y_pred_val[indx] == 1) ]\n",
    "ind_true_neg = [indx for indx,y in enumerate(y_val) if (y_val[indx] == 0) and (y_pred_val[indx] == 0) ]\n",
    "ind_false_neg = [indx for indx,y in enumerate(y_val) if (y_val[indx] == 1) and (y_pred_val[indx] == 0) ]\n",
    "ind_false_pos = [indx for indx,y in enumerate(y_val) if (y_val[indx] == 0) and (y_pred_val[indx] == 1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "print(y_test)\n",
    "plt.plot(np.linspace(25,54,30),model_predictions[n])\n",
    "plt.title('True:'+str(y_test[n])+' Predicted:'+str(y_pred[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Look at memory usage of all items in notebook\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
